{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1Nex5mGcBvupPI-yJVd_muzFdcB-VtXhY","authorship_tag":"ABX9TyNbu9kajdZUFPqEaI+ZHe6Y"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# INTRODUCTION\n","\n","To identify an object in an image, there is need to handle alot of irrelevant information and we only need to predict if the object is present in the image or not irrespective of the image transformation like changing the size (tilt), angle or position (left, right, top, bottom or center). I.e the algorithm should be able to recognize invariant representation of the image (unchanged when a specified transformation is applied). The following are image invariance:\n","* **Scale Invariance:** when the model does not change its prediction irrespective of the size of the object.\n","* **Rotation Invariance:** when angle of the object in the image does not matter.\n","* **Translation Invariance:** when position of the image does not matter (CNN has some built in translation invariance).\n","\n","### Augmentation\n","To make network become insensitive to changes such as rotation, translation, and dilation, perform **augmentation** on the image to generate new images, i.e use the same input image and rotate it, translate it, and scale it and ask the network not to change its prediction!\n","\n","In practice, this is achieved by applying random transformations to the input images before they are fed to the network. Advantages of image augmentation includes:\n","1. Increase the robustness of the network\n","2. Avoid overfitting\n","3. Introduce rotational, translational and scale invariance as well as\n","4. Insensitiveness to color changes\n","5. Avoid shortcut learning"],"metadata":{"id":"8Kvz0TVc-2Qq"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torchvision.transforms as T\n"],"metadata":{"id":"T1ZWj0XfWkgv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Transforming Training Set\n","\n","```{py}\n","train_transforms = T.Compose(\n","    [\n","        # Here let's use 256x256\n","        T.Resize(256),\n","\n","        # Let's apply random affine transformations (rotation, translation, shear)\n","        T.RandomAffine(scale=(0.9, 1.1), translate=(0.1, 0.1), degrees=10),\n","\n","        # Color modifications. Here I exaggerate to show the effect\n","        T.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0.5),\n","\n","        # Apply an horizontal flip with 50% probability (i.e., if you pass\n","        # 100 images through around half of them will undergo the flipping)\n","        T.RandomHorizontalFlip(0.5),\n","\n","        # Finally take a 224x224 random part of the image\n","        T.RandomCrop(224, padding_mode=\"reflect\", pad_if_needed=True),  # -\n","\n","        # convert the to tensor array and normalize them\n","        T.ToTensor(),\n","        T.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n","    ]\n",")\n","```"],"metadata":{"id":"eznUTWUgZegV"}},{"cell_type":"markdown","source":["#### Transforming test and validation set\n","\n","During validation and test you typically do not want to apply image augmentation (which is needed for training). Hence, this is a typical transform pipeline for validation and test that can be paired with the pipeline above:\n","\n","```{py}\n","testval_transforms = T.Compose(\n","    [\n","        # The size here depends on your application. Here let's use 256x256\n","        T.Resize(256),\n","        # Let's take the central 224x224 part of the image\n","        T.CenterCrop(224),\n","        T.ToTensor(),\n","        T.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n","    ]\n",")\n","```"],"metadata":{"id":"AJ4-OOS-ZPdF"}},{"cell_type":"markdown","source":["Note that of course:\n","\n","The resize and crop should be the same as applied during training for best performance\n","The normalization should be the same between training and inference (validation and test)"],"metadata":{"id":"aJYkTEkwZKYT"}},{"cell_type":"markdown","source":["#### AutoAugment Transforms\n","These class (`RandAugment`) implements augmentation policies that have been optimized in a data-driven way, by performing large-scale experiments on datasets such as ImageNet and testing many different recipes, to find the augmentation policy giving the best result. It is then proven that these policies provide good performances also on datasets different from what they were designed for.\n","\n","```{py)\n","T.RandAugment(num_ops, magnitude)\n","```\n","* `num_ops`: the number of random transformations applied. Defaut: 2\n","* `magnitude`: the strength of the augmentations. The larger the value, the more diverse and extreme the augmentations will become."],"metadata":{"id":"VRHe3hI3As1_"}},{"cell_type":"markdown","source":["### Batch Normalization\n","\n","Just as we normalize the input image before feeding it to the network, we would like to keep the feature maps normalized, since they are the output of one layer and the input to the next layer.\n","\n","Here’s a simpler bullet-point summary of the explanation:\n","* Batch Normalization (BatchNorm) does this by:\n","  * Normalizing activations in each mini-batch.\n","  * Keeping values stable so layers don’t need big adjustments.\n","  * Helping training converge faster.\n","  * Added small regulariztion effect by introducing a bit of noise through batch statistics since each batch are different.\n","\n","* During training:\n","  * BatchNorm calculates mean and variance for each mini-batch (therefore batch size must not be too small).\n","  * Needs sufficiently large batch size for accurate estimates.\n","  * Keeps a running average of mean and variance to use later.\n","* During inference:\n","  * BatchNorm uses the stored running averages, not batch data.\n","* Therefore, BatchNorm behaves differently:\n","  * In training mode: `model.train()`\n","  * In evaluation/inference mode: `model.eval()`\n","\n","#### BatctNorm in Pytorch\n","* Applying BatchNorm on a convolutional layer: `nn.BatchNorm2d`\n","```{py}\n","self.conv1 = nn.Sequential(\n","  nn.Conv2d(3, 16, kernel_size=3, padding=1),\n","  nn.BatchNorm2d(16),\n","  nn.MaxPool2d(2, 2),\n","  nn.ReLU(),\n","  nn.Dropout2d(0.2)\n",")\n","```\n","* Applying BatchNorm to MLPs very easily by using: `nn.BatchNorm1d`\n","```{py}\n","self.mlp = nn.Sequential(\n","  nn.Linear(1024, 500),\n","  nn.BatchNorm1d(500),\n","  nn.ReLU(),\n","  nn.Dropout(0.5)\n",")\n","```\n","\n","Kindly note that in bot cases, batchnorm must be applied before dropout if there is dropout in the archotechture"],"metadata":{"id":"QRAMk8hYIY79"}},{"cell_type":"markdown","source":["### Optimizing the Performance of Our Network\n","\n","The following are the strategies for Optimizing Hyperparameters\n","* **Grid search:**\n","  * Divide the parameter space in a regular grid.\n","  * Execute one experiment for each point in the grid\n","  * Simple, but wasteful\n","* **Random search:**\n","  * Divide the parameter space in a random grid\n","  * Execute one experiment for each point in the grid\n","  * Much more efficient sampling of the hyperparameter space with respect to grid search\n","* **Bayesian Optimization:**\n","  * Algorithm for searching the hyperparameter space using a Gaussian Process model\n","  * Efficiently samples the hyperparameter space using minimal experiments\n","\n"],"metadata":{"id":"YWbAepPGgbgy"}},{"cell_type":"markdown","source":["### Tracking Experiments\n","\n","When performing hyperparameter optimization and other changes, it is very important to keep track all of all experiments. This will help to know which hyperparameters have given which results, and to be able to repeat those experiments, choose the best one, understand what works and what doesn't, and what you need to explore further.\n","\n","This experiment and its result can be tracked with **MLFlow**. Tracking an experiment is easy in mlflow. Start by creating a run. A run is a unit of execution that will contain experiment results. Think of it as one row in a hypothetical spreadsheet, where the columns are the things you want to track (accuracy, validation loss, ...). A run can be created like this:\n","```{py}\n","import mlflow\n","\n","with mlflow.start_run():\n","\n","    ... train and validate ...\n","\n","    # Track values for hyperparameters    \n","    mlflow.log_param(\"learning_rate\", learning_rate)\n","    mlflow.log_param(\"batch_size\", batch_size)\n","\n","    # Track results obtained with those values\n","    mlflow.log_metric(\"val_loss\", val_loss)\n","    mlflow.log_metric(\"val_accuracy\", val_accuracy)\n","\n","    # Track artifacts (i.e. files produced by our experiment)\n","    # For example, we can save the weights for the epoch with the\n","    # lowest validation loss\n","    mlflow.log_artifact(\"best_valid.pt\")\n","```"],"metadata":{"id":"Pj_m0Kmw4zGa"}},{"cell_type":"markdown","source":["# IMPROVING MODEL PERFORMANCE\n","\n","I already trained model without augmentation. It is located [here](https://github.com/omogbolahan94/Images-Classification---CNN/blob/main/image_classification.ipynb).\n","\n","To keep things simple, I will optimize two hyperparameters.\n","* For our data augmentation, we are going to use the RandAugment with one parameter, called magnitude. This parameter will be one of the parameters we will optimize in our hyperparameter search.\n","* Optimize the learning rate for the gradient descent."],"metadata":{"id":"ApaJV7P1OtWK"}},{"cell_type":"code","source":["import torch\n","import numpy as np\n","\n","import torchvision.transforms as T\n","from torchvision import datasets\n","from torch.utils.data.sampler import SubsetRandomSampler\n","import random\n","\n","import os\n","os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n","\n","import multiprocessing\n","import torch.multiprocessing\n","torch.multiprocessing.set_sharing_strategy('file_system')"],"metadata":{"id":"QCZ1VAfQPERm","executionInfo":{"status":"ok","timestamp":1751280721535,"user_tz":-60,"elapsed":4398,"user":{"displayName":"Gabriel Olatunji","userId":"09320870513580623361"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["# check if CUDA is available\n","train_on_gpu = torch.cuda.is_available()\n","\n","if not train_on_gpu:\n","    print('CUDA is not available.  Training on CPU ...')\n","else:\n","    print('CUDA is available!  Training on GPU ...')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0Aop_AqHPSsD","executionInfo":{"status":"ok","timestamp":1751280489718,"user_tz":-60,"elapsed":15,"user":{"displayName":"Gabriel Olatunji","userId":"09320870513580623361"}},"outputId":"3e4dc36d-abb5-4aee-bb51-bbbc1ebd4733"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["CUDA is not available.  Training on CPU ...\n"]}]},{"cell_type":"markdown","source":["### Transformer Function\n","\n","Transform the data so we can optimize the hyperparameters"],"metadata":{"id":"OpRqGIYBiTvl"}},{"cell_type":"code","source":["def get_transforms(rand_augment_magnitude):\n","\n","    # These are the per-channel mean and std of CIFAR-10 over the dataset\n","    mean = (0.49139968, 0.48215827, 0.44653124)\n","    std = (0.24703233, 0.24348505, 0.26158768)\n","\n","    # Define our transformations\n","    return {\n","        \"train\": T.Compose(\n","            [\n","                # All images in CIFAR-10 are 32x32. We enlarge them a bit so we can then\n","                # take a random crop\n","                T.Resize(40),\n","\n","                # take a random part of the image\n","                T.RandomCrop(32),\n","\n","                # Horizontal flip is not part of RandAugment according to the RandAugment\n","                # paper\n","                T.RandomHorizontalFlip(0.5),\n","\n","                # Use RandAugment\n","                # RandAugment has 2 main parameters: how many transformations should be\n","                # applied to each image, and the strength of these transformations. This\n","                # latter parameter should be tuned through experiments: the higher the more\n","                # the regularization effect\n","                # Setup a T.RandAugment transformation using 2 as num_opts, and the\n","                # rand_augment_magnitude input parameter as magnitude.\n","                # Use T.InterpolationMode.BILINEAR as interpolation. Look at the pytorch\n","                # manual if needed:\n","                # https://pytorch.org/vision/main/generated/torchvision.transforms.RandAugment.html\n","\n","                T.RandAugment(\n","                    num_ops=2,\n","                    magnitude=rand_augment_magnitude,\n","                    interpolation=T.InterpolationMode.BILINEAR,\n","                ),\n","\n","                T.ToTensor(),\n","                T.Normalize(mean, std),\n","            ]\n","        ),\n","        \"valid\": T.Compose(\n","            [\n","                # Both of these are useless, but we keep them because\n","                # in a non-academic dataset you will need them\n","                T.Resize(32),\n","                T.CenterCrop(32),\n","\n","                # Convert to tensor and apply normalization:\n","                T.ToTensor(),\n","                T.Normalize(mean, std),\n","            ]\n","        ),\n","        # Identical to the valid set in this case\n","        \"test\": T.Compose(\n","            [\n","                T.Resize(32),\n","                T.CenterCrop(32),\n","\n","                # Convert to tensor and apply normalization:\n","                T.ToTensor(),\n","                T.Normalize(mean, std),\n","            ]\n","        ),\n","    }"],"metadata":{"id":"yHgfih2LPgYU","executionInfo":{"status":"ok","timestamp":1751282352780,"user_tz":-60,"elapsed":51,"user":{"displayName":"Gabriel Olatunji","userId":"09320870513580623361"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["### Data Loader Function"],"metadata":{"id":"R4N3MD_rlMrN"}},{"cell_type":"code","source":["def get_data_loaders(batch_size, valid_size, transforms, num_workers, random_seed=42):\n","\n","    # this data is already saved on google drive\n","    cifer10_path = '/content/drive/MyDrive/Deap_Learning/cifar10_data/'\n","\n","    # Reseed random number generators to get a deterministic split. This is useful\n","    # when comparing experiments, so you'll know they all run on the same data.\n","    # In principle you should repeat this a few times (cross validation) to see\n","    # the variability of your measurements, but we won't do this here for simplicity\n","    torch.manual_seed(random_seed)\n","    random.seed(random_seed)\n","    np.random.seed(random_seed)\n","\n","    # Get the CIFAR10 training dataset from torchvision.datasets and set the transforms\n","    # We will split this further into train and validation in this function\n","    train_data = datasets.CIFAR10(cifer10_path, train=True, download=False, transform=transforms['train'])\n","    valid_data = datasets.CIFAR10(cifer10_path, train=True, download=False, transform=transforms['valid'])\n","\n","    # Compute how many items we will reserve for the validation set\n","    n_tot = len(train_data)\n","    split = int(np.floor(valid_size * n_tot))\n","\n","    # compute the indices for the training set and for the validation set\n","    shuffled_indices = torch.randperm(n_tot)\n","    train_idx, valid_idx = shuffled_indices[split:], shuffled_indices[:split]\n","\n","    # define samplers for obtaining training and validation batches\n","    train_sampler = SubsetRandomSampler(train_idx)\n","    valid_sampler = SubsetRandomSampler(valid_idx)\n","\n","    # prepare data loaders (combine dataset and sampler)\n","    # NOTE that here we use train_data for the train dataloader but valid_data\n","    # for the valid_loader, so the respective transforms are applied\n","    train_loader = torch.utils.data.DataLoader(\n","        train_data, batch_size=batch_size, sampler=train_sampler, num_workers=num_workers\n","    )\n","    valid_loader = torch.utils.data.DataLoader(\n","        valid_data, batch_size=batch_size, sampler=valid_sampler, num_workers=num_workers\n","    )\n","\n","    test_data = datasets.CIFAR10(\"data\", train=False, download=False, transform=transforms['test'])\n","    test_loader = torch.utils.data.DataLoader(\n","        test_data, batch_size=batch_size, num_workers=num_workers\n","    )\n","\n","    return {'train': train_loader, 'valid': valid_loader, 'test': test_loader}"],"metadata":{"id":"FDcuUxK6kAsZ","executionInfo":{"status":"ok","timestamp":1751282661939,"user_tz":-60,"elapsed":21,"user":{"displayName":"Gabriel Olatunji","userId":"09320870513580623361"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["### Data Classes (categories)"],"metadata":{"id":"0-jtKVbxlTSu"}},{"cell_type":"code","source":["# specify the image classes\n","classes = [\n","    \"airplane\",\n","    \"automobile\",\n","    \"bird\",\n","    \"cat\",\n","    \"deer\",\n","    \"dog\",\n","    \"frog\",\n","    \"horse\",\n","    \"ship\",\n","    \"truck\",\n","]"],"metadata":{"id":"1LdRKPYFlRjX","executionInfo":{"status":"ok","timestamp":1751282766113,"user_tz":-60,"elapsed":43,"user":{"displayName":"Gabriel Olatunji","userId":"09320870513580623361"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"XKbxD_eBllbj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# GIT TRACKING"],"metadata":{"id":"lS4zrCST8L52"}},{"cell_type":"code","source":["!pip install python-dotenv --quiet"],"metadata":{"id":"hCCgYeTS8RTk","executionInfo":{"status":"ok","timestamp":1751282716773,"user_tz":-60,"elapsed":6886,"user":{"displayName":"Gabriel Olatunji","userId":"09320870513580623361"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["from dotenv import load_dotenv\n","import os"],"metadata":{"id":"9GAqvbkn8X28","executionInfo":{"status":"ok","timestamp":1751282937842,"user_tz":-60,"elapsed":5,"user":{"displayName":"Gabriel Olatunji","userId":"09320870513580623361"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["notebook_name = \"image_augmentation.ipynb\"\n","repo_name = \"image-augmentation\"\n","git_username = \"omogbolahan94\"\n","email = \"gabrielomogbolahan1@gmail.com\""],"metadata":{"id":"NWlJUUJQ8a_1","executionInfo":{"status":"ok","timestamp":1751283359555,"user_tz":-60,"elapsed":5,"user":{"displayName":"Gabriel Olatunji","userId":"09320870513580623361"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["def push_to_git(notebook_name, repo_name, commit_m, git_username, email):\n","  token_path = '/content/drive/MyDrive/Environment-Variable/variable.env'\n","  load_dotenv(dotenv_path=token_path)\n","  GITHUB_TOKEN = os.getenv('GIT_TOKEN')\n","\n","  USERNAME = f\"{git_username}\"\n","  REPO = f\"{repo_name}\"\n","\n","  # Authenticated URL\n","  remote_url = f\"https://{USERNAME}:{GITHUB_TOKEN}@github.com/{USERNAME}/{REPO}.git\"\n","  if REPO not in os.listdir():\n","    !git clone {remote_url}\n","\n","  # copy notebook to the cloned CNN\n","  notebook_path = f\"/content/drive/My Drive/Colab Notebooks/{notebook_name}\"\n","  !cp '{notebook_path}' '/content/{REPO}/'\n","\n","  # ensure to be in the repository folder\n","  %cd '/content/{REPO}'\n","\n","  # copy the saved model into the cloned repository\n","  if \"cifar10_best_valid.pt\" not in os.listdir():\n","    if os.path.exists('/content/cifar10_best_valid.pt'):\n","      !cp /content/cifar10_best_valid.pt /content/{REPO}/\n","  if 'cifar10_network.pt' not in os.listdir():\n","    if os.path.exists('/content/cifar10_best_valid.pt'):\n","      !cp /content/cifar10_network.pt /content/{REPO}/\n","\n","  # Reconfigure Git\n","  !git config --global user.name '{USERNAME}'\n","  !git config --global user.email '{email}'\n","  !git remote set-url origin '{remote_url}'\n","\n","  print()\n","  !git add .\n","  !git commit -m '{commit_m}'\n","  !git push origin main\n","\n","  # change back to the content directory\n","  %cd '/content'"],"metadata":{"id":"uPvfR5yN82Dz","executionInfo":{"status":"ok","timestamp":1751283324324,"user_tz":-60,"elapsed":32,"user":{"displayName":"Gabriel Olatunji","userId":"09320870513580623361"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["commit_m = \"data loader and transformation object\""],"metadata":{"id":"ukyXFC1O81DA","executionInfo":{"status":"ok","timestamp":1751283337157,"user_tz":-60,"elapsed":41,"user":{"displayName":"Gabriel Olatunji","userId":"09320870513580623361"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["push_to_git(notebook_name, repo_name, commit_m, git_username, email)"],"metadata":{"id":"QLPrxy5H850Y","colab":{"base_uri":"https://localhost:8080/","height":141},"executionInfo":{"status":"error","timestamp":1751283338626,"user_tz":-60,"elapsed":10,"user":{"displayName":"Gabriel Olatunji","userId":"09320870513580623361"}},"outputId":"15e8deab-8768-4b0e-b41b-437754f26906"},"execution_count":12,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'notebook_name' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-12-162533800.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpush_to_git\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnotebook_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepo_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcommit_m\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgit_username\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memail\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'notebook_name' is not defined"]}]},{"cell_type":"code","source":[],"metadata":{"id":"Got2zS2YMRMs"},"execution_count":null,"outputs":[]}]}